<!doctype html>
<html>
<head>
<link rel="icon" href="images/man_1.png" type="image/x-icon">
<meta charset="gbk">
<title>Songlin Xu's Website</title>
<meta name="keywords" content="Songlin Xu's Website" />
<meta name="description" content="Songlin Xu's Website" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="css/base.css" rel="stylesheet">
<link href="css/index.css" rel="stylesheet">
<link href="css/m.css" rel="stylesheet">
<script src="js/jquery.min.js" type="text/javascript"></script>
<script src="js/jquery.easyfader.min.js"></script>
<script src="js/scrollReveal.js"></script>
<script src="js/common.js"></script>
<!--[if lt IE 9]>
<script src="js/modernizr.js"></script>
<![endif]-->
</head>
<body>
<header> 
  <!--menu begin-->
  <div class="menu">
    <nav class="nav" id="topnav">
      <h1 class="logo"><a href="index.html">Songlin Xu</a></h1>
      
      
    </nav>
  </div>
  <!--menu end--> 
  
</header>
<article> 
  <!--banner begin-->
 <div class="picsbox"> 
  <div class="banner">
    <div id="banner" class="fader">
      <li class="slide" ><a href="images/man_2.png" target="_blank"><div><img src="images/man_2.png"></div><span class="imginfo">My research tackles a wide range of 
        smart wearables for HCI and Ubicomp.
      </span></a></li>
      <li class="slide" ><a href="projects/Hydrauio/Hydrauio2.jpg" target="_blank"><div><img src="projects/Hydrauio/Hydrauio2.jpg"></div><span class="imginfo">UIST 2020: Hydrauio: a novel pen interface to support gesture input and haptic feedback.</span></a></li>
      <li class="slide" ><a href="projects/FingerTrak/FingerTrak.jpg" target="_blank"><img src="projects/FingerTrak/FingerTrak.jpg"><span class="imginfo">IMWUT 2020: FingerTrak: a smart wristband to reconstruct hand poses.</span></a></li>
      
      <li class="slide" ><a href="rojects/SpringErr/SpringErr.png" target="_blank"><img src="projects/SpringErr/SpringErr.png"><span class="imginfo">SpringErr: A Haptic Feedback System for Breadboards Using Virtual Springs</span></a></li>
      <li class="slide" ><a href="images/all_robot.jpg" target="_blank"><img src="images/all_robot.jpg"><span class="imginfo">Some robots~</span></a></li>
     <div class="fader_controls">
        <div class="page prev" data-target="prev">&lsaquo;</div>
        <div class="page next" data-target="next">&rsaquo;</div>
        <ul class="pager_list">
        </ul>
      </div>
    </div>
  </div>
  </div>

  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle">About Me</h3>
      <p class="blogtext">I'm an undergraduate from USTC. My current research investigates: <br><br>
        1. How to utilize hands-free gestures to enable text entry; 
      </p>
      <p class="blogtext">
        2. Introducing new input(sensing) and output(haptic/thermal feedback) techniques to enable novel interactions on smart wearables; 
      </p>
      <p class="blogtext">
        3. Sensing for human activity recognition and sensing in the environment.
      </p>
      <br><br>
      <h3 class="blogtitle"><a href="/"> CV is available upon request.</a></h3>
      

    </div>
  </div>
   
  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle">Skill Sets</h3>
      <p class="blogtext">
      <li><b>Programming Languages</b>: 
          <br>Proficient in: Python, C, Matlab, Arduino, LaTeX
          <br>Also basic ability with: processing, VHDL.</li>
      <br>
      <li><b>Industry Software Skills</b>:
          <br>SolidWorks, AutoCAD, RDworks, Matlab, Photoshop
      </li>
      <br>
      <li><b>Research Skills</b>:
          <br>Fast prototyping, Coding, Mechanical design, Electronic design, Machine learning algorithm development, Paper writing
      </li>
      <br>
      <li><b>General Skills</b>:
          <br>Good hands-on skills, Works well in a team, Works hard, Presentation skills</li>
      </p>
    </div>
  </div>

  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle">Contact Me</h3>
      <p class="blogtext">songlinxu6@outlook.com
      </p>
    </div>
  </div>
  

  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle">News</h3>
      <p class="blogtext">
        
        <p>2020.10.9: One long paper is submitted to <b>IUI 2021</b>.</p>
        <br>
        <p>2020.8.6: Hydrauio is accepted by <b>UIST 2020 Poster</b>!</p>
        <br>
        <p>2020.7.1: Graduate from USTC. Goodbye, USTC!</p>
        <br>
        <p>2020.6.11: FingerTrak is accepted by <b>IMWUT 2020</b>!</p>
        <br>
        <p>2019.7-2020.2: Internship and conducting my graduation thesis research at Cornell University(Advisor: Prof. Cheng Zhang)</p>
        <br>
        <p>2019.8: One paper got accepted by WRC SARA 2019 and won the best paper finalist award!</p>
        <br>
        <p>2019.6: Excellent Project Award in Student Innovation Program of USTC</p>
        <br>
        <p>2019.5: The third-class prize of Student Innovation Program at Chinese Academy of Sciences</p>
        <br>
        <p>2019.1-2019.3: Internship at Cornell University(Advisor: Prof. Cheng Zhang)</p>
        <br>
        <p>2019.1: One paper got accepted by IEEE Robotics and Automation Letter!</p>
        <br>
        <p>2018.8-2018.9: Internship at Carnegie Mellon University(Advisor: Prof. Ding Zhao)</p>
        <br>
        <p>2018.7-2018.8: Internship at University of Michigan -- Ann arbor(Advisor: Prof. Ding Zhao)</p>
        <br>
        <p>2018.6: The second-class prize in Robocon of China</p>
        <br>
        <p>2018.1-2018.3: Internship at Dartmouth College(Advisor: Prof. Xing-Dong Yang)</p>
        <br>
        <p>2017.10: Joined the Robotics Lab at USTC under the supervision of Prof. Xiaoping Chen</p>
        <br>
        <p>2017.10: The third-class prize in the divisional competition of China Aeromodelling Design Challenge</p>
        <br>
        <p>2017.9: The second place of 2017RoboGame at USTC</p>
        
      </p>
    </div>
  </div>


    <br><br>
<!--blogsbox begin-->
   <br>
   <br>
   <p>
   	<h1>
   		&nbsp;&nbsp;Current Research
   	</h1>
   </p>
   <br>
   <br>
<div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle"><a href="research.html">Text Entry using Hands-free Body Gestures. </a></h3>
      <span class="blogpic"><a href="research.html" title=""></a></span>
      <p class="blogtext">
          Hands-free input is useful, especially when users' hands are busy with other tasks. My current research investigates developing novel hands-free interaction techniques
          to enable text entry as well as other input scenarios.
      </p>
    </div>
</div>

<div class="blogs" data-scroll-reveal="enter bottom over 1s" >
  <h3 class="blogtitle"><a href="research.html">Novel Interactions on Smart Wearables. </a></h3>
  <span class="blogpic"><a href="research.html" title=""></a></span>
  <p class="blogtext">
      Smart wearables are popular today. Whereas existing technologies could not provide natural enough interaction experience to users. My research aims to pave the way for a more
      smooth, natural and convenient interaction experience through the development of novel input(sensing) and output(haptics) techniques.
  </p>
</div>
</div>

<div class="blogs" data-scroll-reveal="enter bottom over 1s" >
  <h3 class="blogtitle"><a href="research.html">Sensing for Human Activity Recognition and Sensing in the Environment. </a></h3>
  <span class="blogpic"><a href="research.html" title=""></a></span>
  <p class="blogtext">
      Human activity recognition(e.g. body gestures) is useful to assist input and enhance user experience. Aside from sensing for humans, sensing in the environment offers a wider way to 
      support a more natural interaction experience.
  </p>
</div>
</div>



  	<br>
  	<hr>
  	<p>
   	<h1>
   		&nbsp;&nbsp;Publications: Human Computer Interaction and Ubiquitous Computing
   	</h1>
   </p>

   
   <hr>
   <br>

   <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle"><a href="https://dl.acm.org/doi/10.1145/3397306">FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist</a></h3>
      <span class="blogpic"><img src="projects/FingerTrak/FingerTrak.jpg" alt=""></span>
      <p class="blogtext">In this paper, we present FingerTrak, a minimal-obtrusive wristband that enables continuous 3D finger tracking and hand pose estimation with four miniature thermal cameras mounted closely on a form-fitting wristband. 
        FingerTrak explores the feasibility of continuously reconstructing the entire hand postures (20 finger joints positions) without the needs of seeing all fingers. 
      </p>
      <div class="bloginfo">
        <ul>
          <li class="author">Fang Hu, Peng He, <b>Songlin Xu</b>, Yin Li and Cheng Zhang</li>
          <br>
          <li class="lmname"><b>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. June 2020</b></li>
          <br>
          <li class="view"><a href="projects/FingerTrak/FingerTrak.mov">Video</a></li>
          <li class="like"><a href="projects/FingerTrak/FingerTrak.pdf">Pdf</a></li>
          <li class="timer">2020</li>
        </ul>
      </div>
    </div>
  </div>

  

  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle"><a href="projects/Hydrauio/Hydrauio.pdf">Hydrauio: Extending Interaction Space on the Pen through Hydraulic Sensing and Haptic Output</a></h3>
      <span class="blogpic"><img src="projects/Hydrauio/Hydrauio2.jpg" alt=""></span>
      <p class="blogtext">We have explored a fluid-based interface(Hydrauio) on the pen body to extend interaction space of human-pen interaction. 
        Users could perform finger gestures on the pen for input and also receive haptic feedback of different profiles from the fluid surface. 
        The user studies showed that Hydrauio could achieve an accuracy of more than 92% for finger gesture recognition and users could distinguish 
        different haptic profiles with an accuracy of more than 95%. Finally, we present application scenarios to demonstrate the potential of Hydrauio to extend interaction space of human-pen interaction.
      </p>
      <div class="bloginfo">
        <ul>
          <li class="author"><b>Songlin Xu</b>, Zhiyuan Wu, Shunhong Wang, Rui Fan and Nan Lin</li>
          <br>
          <li class="lmname"><b>UIST 2020, Adjunct</b></li>
          <br>
          <li class="view"><a href="projects/Hydrauio/Hydrauio.mp4">Video</a></li>
          <li class="like"><a href="projects/Hydrauio/Hydrauio.pdf">Pdf</a></li>
          <li class="timer">2020</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle"><a href="projects/SpringErr/SpringErr.pdf">SpringErr: A Haptic Feedback System for Breadboards Using Virtual Springs</a></h3>
      <span class="blogpic"><img src="projects/SpringErr/SpringErr.png" alt=""></span>
      <p class="blogtext">This paper introduces a novel haptic feedback method on breadboards. When inserting a component, a user feels a virtual spring underneath pins, whose stiffness varies to inform the user about different types 
        of electronics errors (e.g. bad connections). We demonstrate the potential of this new haptic output through SpringErr, a haptic prototype. We used our prototype to examine the recognizability of six distinct virtual spring profiles, 
        including High Stiffness, Medium Stiffness, Low Stiffness, Bump, Pierce, and Disappear. 
      </p>
      <div class="bloginfo">
        <ul>
          
          <br>
          <li class="lmname"><b>CHI 2019, rejected</b></li>
          <br>
          <li class="view"><a href="projects/SpringErr/SpringErr.mp4">Video</a></li>
          <li class="like"><a href="projects/SpringErr/SpringErr.pdf">Pdf</a></li>
          <li class="timer">2019</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="blogsbox">
    <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
      <h3 class="blogtitle"><a href="/">More Publications to Appear... Feel Free to Contact Me to Learn More.</a></h3>
    </div>
  </div>
 
  
    <br>
  	<hr>
    <p>
      <h1>
        &nbsp;&nbsp;Publications: Robotics
      </h1>
    </p>

    <hr>
    <br>
     
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="https://ieeexplore.ieee.org/document/8931910">Exploring Hardness and Geometry Information through Active Perception</a></h3>
       <span class="blogpic"><a href="images/hardness_cloud2.png" title=""><img src="images/hardness_cloud2.png" alt=""></a></span>
       <p class="blogtext">In this paper, a framework
 combining active perception and motion planning algorithm
 is proposed to get both hardness and geometry information
 of an object which also ensures working efficiency. In this
 framework, a stylus mounted on a robotic arm explores
 hardness and geometry information on the surface of the
 object actively and a depth camera is used to capture raw
 3D shape information. A novel motion planning algorithm is
 proposed to keep the exploration operative and time-saving.
 Experimental results show that our framework has good
 performance and can explore global hardness and geometry
 information efficiently.
 </p>
       <div class="bloginfo">
         <ul>
           <li class="author"><b>Songlin Xu</b>
             , Nan Lin
             , Rui Fan
             , Peichen Wu
             and Xiaoping Chen</li>
           <br>
           <li class="lmname"><a href="https://ieeexplore.ieee.org/document/8931910">WRC SARA 2019(<b>Best Paper Finalist Award</b>)</a></li>
           <br>
           <li class="view"><a href="projects/Active Perception/active_perception.mp4">Video</a></li>
           <li class="like"><a href="projects/Active Perception/active perception best paper.pdf">Pdf</a></li>
           <li class="timer">2019</li>
         </ul>
       </div>
     </div>
   </div>
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="active safe control.html">IMU-Based Active Safe Control of a Variable Stiffness Soft Actuator</a></h3>
       <span class="blogpic"><a href="active safe control.html" title=""><img src="images/IMU-based1.jpg" alt=""></a></span>
       <p class="blogtext">In this paper, a novel soft actuator is presented, whose stiffness
         is tunable in multiple ways, and more than a 10-fold stiffness
         enhancement is achievable, making it able to carry heavy loads
         while maintaining excellent dexterity and compliance. Meanwhile, we first proposed an active safe control strategy based on
         inertial measurement units (IMUs). </p>
       <div class="bloginfo">
         <ul>
           <li class="author"><a href="active safe control.html">Nan Lin, Peichen Wu, Menghao Wang, Jizhou Wei, Fan Yang, <b>Songlin Xu</b>, Zhicheng Ye and Xiaoping Chen</a></li>
           <br>
           <li class="lmname"><a href="active safe control.html">IEEE Robotics and Automation Letter</a></li>
           <br>
           <li class="view"><a href="active safe control.html">Video</a></li>
           <li class="like"><a href="active safe control.html">Pdf</a></li>
           <li class="timer">2018</li>
         </ul>
       </div>
     </div>
   </div>
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="Autonomous Vehicles.html">Estimating Risk Levels of Driving Scenarios through Analysis of Driving Styles for Autonomous Vehicles</a></h3>
       <span class="blogpic"><a href="Autonomous Vehicles.html" title=""><img src="images/estimating_driving_4.png" alt=""></a></span>
       <p class="blogtext">In order to operate safely on the road, autonomous
         vehicles need not only to be able to identify objects in front of
         them, but also to be able to estimate the risk level of the object
         in front of the vehicle automatically. It is obvious that different
         objects have different levels of danger to autonomous vehicles.
         An evaluation system is needed to automatically determine
         the danger level of the object in front of the autonomous
         vehicle. It would be too subjective and incomplete if the
         system were completely defined by humans.</p>
       <div class="bloginfo">
         <ul>
           <li class="author"><a href="Autonomous Vehicles.html"><b>Songlin Xu</b> and Jiacheng Zhu</a></li>
           <br>
           <li class="lmname"><a href="Autonomous Vehicles.html">Arxiv</a></li>
           <br>
           <li class="view"><a href="Autonomous Vehicles.html">Video</a></li>
           <li class="like"><a href="Autonomous Vehicles.html">Pdf</a></li>
           <li class="timer">2018</li>
         </ul>
       </div>
     </div>
   </div>
 
   
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="Single Pixel Camera.html">Single Pixel Imaging System Using Deep Learning Algorithms for Image Reconstruction</a></h3>
       <span class="blogpic"><a href="Single Pixel Camera.html" title=""><img src="images/single_pixel.png" alt=""></a></span>
       <p class="blogtext">A novel time series inspired end-to-end neural network combining convolutional neural nerwork and long short-term memory neural network(E2E-LSTM-CNN) is
          proposed to reduce the recovery time with less sampling. Our E2E-LSTM-CNN can recover 128&times;128 pixel image within 
          the recovery time of less than 0.5 second from a single-pixel camera sampling at a compression ratio of less 
          than 0.1%. The experiment shows that our method needs less  recovery time than the state-of-art methods but can 
          achieve similar  PSNR and SSIM metric value under less  sampling than other methods </p>
       <div class="bloginfo">
         <ul>
           <li class="author"><a href="Single Pixel Camera.html"></a><b>Songlin Xu</b>, Yangding Peng, Yue Chen, Liqun He and Rongde Lu</li>
           <br>
           <li class="lmname"><a href="Single Pixel Camera.html"></a> &nbsp;</li>
           <br>
           <li class="view"><a href="Single Pixel Camera.html">Video</a></li>
           <li class="like"><a href="Single Pixel Camera.html">Pdf</a></li>
           <li class="timer">2018-2019</li>
         </ul>
       </div>
     </div>
   </div>
 

   <br>
  	<hr>
    <p>
      <h1>
        &nbsp;&nbsp;Fun Projects
      </h1>
    </p>

    <hr>
    <br>


   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="ROV.html">Remote Control Underwater Vehicle</a></h3>
       <span class="blogpic"><a href="ROV.html" title=""><img src="images/rov.jpg" alt=""></a></span>
       <p class="blogtext">We designed a remoted control underwater vehicle based on Ardusub system. A camera is mounted on the 
         ROV to capture videos underwater and a contact microphone is attached on the body of the ROV to capture sound print signals
          while the ROV is operating in the water.
       </p>
       <div class="bloginfo">
         <ul>
           <li class="author"><a href="ROV.html"><b>Songlin Xu</b>, Yingjie Zhang, Jingyi Chen</a></li>
           <br>
           <li class="lmname"><a href="ROV.html">Science and technology innovation program of Chinese Academy of Sciences</a></li>
           <br>
           <li class="view"><a href="ROV.html">Video</a></li>
         
           <li class="timer">2019</li>
         </ul>
       </div>
     </div>
   </div>
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="robogame.html">Drawing Robot in the Robotics Competition</a></h3>
       <span class="blogpic"><a href="robogame.html" title=""><img src="images/robogame_2.jpg" alt=""></a></span>
       <p class="blogtext">Participating in 2017RoboGame which is a robot competition at USTC and making a robot which can draw almost everything if you input its black-and-white photograph and can recognize the characters in the photo and write it in another typeface called Xiaozhuan, an ancient style of calligraphy.</p>
       <div class="bloginfo">
         <ul>
           <li class="author"><a href="robogame.html"><b>Songlin Xu</b>, Bao Li, Yanding Peng, Lin Liu, Jiale Han</a></li>
           <br>
           <li class="lmname"><a href="robogame.html">2017RoboGame Competition</a></li>
           <br>
           <li class="view"><a href="robogame.html">Video</a></li>
           
           <li class="timer">2017</li>
         </ul>
       </div>
     </div>
   </div>
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="cadc.html">China Aeromodelling Design Challenge</a></h3>
       <span class="blogpic"><a href="cadc.html" title=""><img src="images/cadc4.jpg" alt=""></a></span>
       <p class="blogtext">Taking part in the China Aeromodelling Design Challenge and designing and making a kind of model airplane which is lighter than 1 kg but can hold more than 1 kg of water and then throw the bag containing water into the right area accurately.</p>
       <div class="bloginfo">
         <ul>
           <li class="author"><a href="cadc.html">USTC team</a></li>
           <br>
           <li class="lmname"><a href="cadc.html">China Aeromodelling Design Challenge</a></li>
           <li class="timer">2017</li>
         </ul>
       </div>
     </div>
   </div>
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="robocon.html">Robocon</a></h3>
       <span class="blogpic"><a href="robocon.html" title=""><img src="images/robocon.jpg" alt=""></a></span>
       <p class="blogtext">Participating in Robocon of China in 2018 and I am in the computer vision group of USTC. Our
           job is to identify whether the shuttlecock has entered the ring in a complex environment. And we
           get the second-class prize on behalf of USTC.</p>
       <div class="bloginfo">
         <ul>
          
             <li class="author"><a href="robocon.html">USTC team</a></li>
 
             <br>
           <li class="lmname"><a href="robocon.html">Robocon in China</a></li>
           <li class="timer">2018</li>
         </ul>
       </div>
     </div>
   </div>
 
   <div class="blogsbox">
     <div class="blogs" data-scroll-reveal="enter bottom over 1s" >
       <h3 class="blogtitle"><a href="uav.html">Making a Novel UAV</a></h3>
       <span class="blogpic"><a href="uav.html" title=""><img src="images/uav.jpg" alt=""></a></span>
       <p class="blogtext">Making a new kind of UAV based on the four-rotor aircraft which can sail on the water by changing the degree of rotation of a shaft which is connected with two motors.</p>
       <div class="bloginfo">
         <ul>
          
           <li class="view"><a href="uav.html">Video</a></li>
          
           <li class="timer">2017</li>
         </ul>
       </div>
     </div>
   </div>
  
   

    
    
   
  
   
  
  
</article>
<footer>
  <p>Copyright by Songlin Xu</a></p>
</footer>
<a href="#" class="cd-top">Top</a>
</body>
</html>
